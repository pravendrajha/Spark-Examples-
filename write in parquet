import org.apache.spark.sql.SQLContext
import org.apache.spark._
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Row
import java.util.Properties
import com.mongodb.spark.MongoSpark
import com.mongodb.spark.config.{ ReadConfig, WriteConfig }
import org.apache.spark.sql.SQLContext
import org.apache.spark.{ SparkConf, SparkContext }


object sparkSQL1 {

  def main(args: Array[String]) {

    val spark = SparkSession
      .builder().master("local")
      .appName("SparkSessionZipsExample")
      .enableHiveSupport()
      .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/audit_trail.JobDetailsHistory?readPreference=primaryPreferred")
      .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/audit_trail.spark_data-mongo")
      .config("spark.executor.cores", "4")
      .config("spark.executor.memory", "2g")
      .getOrCreate()

val df = MongoSpark.load(spark)  // Uses the SparkSession
df.printSchema()                        

import com.databricks.spark.avro._

import spark.implicits._


println ( "after spark.implicits._ ") 

df.show()


df.coalesce(1).write.mode("overwrite").parquet("/user/pravjha/avro_write_parquest")

val readparqdata = spark.read.parquet("/user/pravjha/avro_write_parquest")

readparqdata.printSchema()

readparqdata.show()


}
}
