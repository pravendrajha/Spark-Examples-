import org.apache.spark.sql.SQLContext
import org.apache.spark._
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Row
import java.util.Properties
import com.mongodb.spark.MongoSpark

object sparkSQL1 {
  
def main( args:Array[String])
{

val spark = SparkSession
   .builder().master("local")
   .appName("SparkSessionZipsExample")
   .enableHiveSupport()
   .getOrCreate()

import spark.implicits._

spark.conf.set("spark.executor.cores", "4")
spark.conf.set("spark.executor.memory", "2g")

val inFileRDD = spark.sparkContext.textFile( "file:///C:/Users/pravjha/Desktop/hadoop_poc/datat/Online_Retail.txt")


val allRowsRDD = inFileRDD.map( line=>line.split("\t").map(_.trim))


val header = allRowsRDD.first
val data = allRowsRDD.filter(_(0) != header(0))
val fields = Seq(
StructField("invoiceNo", StringType, true),
StructField("stockCode", StringType, true),
StructField("description", StringType, true),
StructField("quantity", IntegerType, true),
StructField("invoiceDate", StringType, true),
StructField("unitPrice", DoubleType, true),
StructField("customerID", StringType, true),
StructField("country", StringType, true)
)
val schema = StructType(fields)
val rowRDD = data.map(attributes => Row(attributes(0), attributes(1), attributes(2), attributes(3).toInt, attributes(4), attributes(5).toDouble, attributes(6), attributes(7)))
val r1DF = spark.createDataFrame(rowRDD, schema)

r1DF.show()

r1DF.write.format("json").save("/user/pravjha/hdfs/data")

}
}
