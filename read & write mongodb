import org.apache.spark.sql.SQLContext
import org.apache.spark._
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions
import org.apache.spark.sql.Row
import java.util.Properties
import com.mongodb.spark.MongoSpark
import com.mongodb.spark.config.{ ReadConfig, WriteConfig }
import org.apache.spark.sql.SQLContext
import org.apache.spark.{ SparkConf, SparkContext }

object sparkSQL1 {

  def main(args: Array[String]) {

    val spark = SparkSession
      .builder().master("local")
      .appName("SparkSessionZipsExample")
      .enableHiveSupport()
      .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/audit_trail.JobDetailsHistory?readPreference=primaryPreferred")
      .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/audit_trail.spark_data-mongo")
      .config("spark.executor.cores", "4")
      .config("spark.executor.memory", "2g")
      .getOrCreate()


val df = MongoSpark.load(spark)  // Uses the SparkSession
df.printSchema()                        

df.filter("job_name =='test1'")

MongoSpark.save(df.write.option("collection", "spark_data_mongo").mode("overwrite"))

}
}
